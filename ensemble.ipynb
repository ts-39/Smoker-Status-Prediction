{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1329443b-97b6-42e2-a01c-e8de0255e242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "import torch\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "import re\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "def seed_everything(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "seed_everything()\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"DEVICE:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0deb74b7-6539-4134-a1c4-4df368986b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original training data had 15000 rows.\n",
      "Our new combined training data has 48467 rows.\n",
      "Our final test data has 10000 rows to predict.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"data\"\n",
    "DATA2_PATH = \"data2\"\n",
    "\n",
    "try:\n",
    "    train_data = pd.read_csv(os.path.join(DATA_PATH, \"train.csv\"))\n",
    "    test_data = pd.read_csv(os.path.join(DATA_PATH, \"test.csv\"))\n",
    "\n",
    "    train_data2 = pd.read_csv(os.path.join(DATA2_PATH, \"train_dataset.csv\"))\n",
    "    test_data2 = pd.read_csv(os.path.join(DATA2_PATH, \"test_dataset.csv\"))\n",
    "\n",
    "    test_ids = test_data['id']\n",
    "    \n",
    "    train_data = train_data.drop('id', axis=1)\n",
    "    test_data = test_data.drop('id', axis=1)\n",
    "\n",
    "    def clean_col_names(df):\n",
    "        cols = df.columns\n",
    "        new_cols = []\n",
    "        for col in cols:\n",
    "            new_col = re.sub(r'[^A-Za-z0-9_]+', '', col)\n",
    "            new_cols.append(new_col.lower())\n",
    "        df.columns = new_cols\n",
    "        return df\n",
    "\n",
    "    train_data = clean_col_names(train_data)\n",
    "    test_data = clean_col_names(test_data)\n",
    "    train_data2 = clean_col_names(train_data2)\n",
    "    test_data2 = clean_col_names(test_data2)\n",
    "\n",
    "\n",
    "    df_train = pd.concat([train_data, train_data2, test_data2], ignore_index=True)\n",
    "    df_test = test_data\n",
    "    \n",
    "    df_train = df_train.drop_duplicates()\n",
    "\n",
    "    df_train.dropna(subset=['smoking'], inplace=True)\n",
    "    \n",
    "    df_train['smoking'] = df_train['smoking'].astype(int)\n",
    "\n",
    "    print(f\"Original training data had {train_data.shape[0]} rows.\")\n",
    "    print(f\"Our new combined training data has {df_train.shape[0]} rows.\")\n",
    "    print(f\"Our final test data has {test_data.shape[0]} rows to predict.\\n\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Couldn't find the files. Check this path: {DATA_PATH} and {DATA2_PATH}\")\n",
    "\n",
    "\n",
    "TARGET = \"smoking\"\n",
    "DROP_COLS = [\"id\"]\n",
    "\n",
    "df_train[TARGET] = df_train[TARGET].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a2569c2-a984-4e26-aaef-2a71a2b04926",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    df[\"bmi\"] = df[\"weightkg\"] / (df[\"heightcm\"] / 100) ** 2\n",
    "\n",
    "    df[\"vai\"] = (\n",
    "        (df[\"waistcm\"] / 100) *\n",
    "        df[\"bmi\"] *\n",
    "        (df[\"triglyceride\"] / 150) *\n",
    "        (40 / (df[\"hdl\"] + 1e-6))\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "LOG_COLS = [\"triglyceride\", \"gtp\", \"ast\", \"alt\", \"ldl\"]\n",
    "\n",
    "def add_log_features(df):\n",
    "    df = df.copy()\n",
    "    for col in LOG_COLS:\n",
    "        df[f\"log_{col}\"] = np.log1p(df[col])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7a84331-9ce8-4c7d-8509-a2a8befeae76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_fe = add_features(df_train)\n",
    "df_test_fe  = add_features(df_test)\n",
    "\n",
    "df_train_fe = add_log_features(df_train_fe)\n",
    "df_test_fe  = add_log_features(df_test_fe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3207859-018d-4612-ba23-c2a8cd691971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- TabNet 用（logのみ使う） ---\n",
    "TABNET_LOG_COLS = [f\"log_{c}\" for c in LOG_COLS]\n",
    "\n",
    "FEATURE_COLS_TABNET = [\n",
    "    c for c in df_train_fe.columns\n",
    "    if c not in DROP_COLS + [TARGET] + LOG_COLS\n",
    "] + TABNET_LOG_COLS\n",
    "\n",
    "\n",
    "# --- LightGBM 用（生 + log） ---\n",
    "FEATURE_COLS_LGB = [\n",
    "    c for c in df_train_fe.columns\n",
    "    if c not in DROP_COLS + [TARGET]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a944ae7-105e-45ee-8a61-5cc6dfb1b0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rankgauss_fold(train_df, valid_df, test_df, cols):\n",
    "    qt = QuantileTransformer(\n",
    "        n_quantiles=1000,\n",
    "        output_distribution=\"normal\",\n",
    "        random_state=SEED\n",
    "    )\n",
    "    qt.fit(train_df[cols].values)\n",
    "\n",
    "    train_df[cols] = qt.transform(train_df[cols].values)\n",
    "    valid_df[cols] = qt.transform(valid_df[cols].values)\n",
    "    test_df[cols]  = qt.transform(test_df[cols].values)\n",
    "\n",
    "    return train_df, valid_df, test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1432b512-9e14-404a-8fd1-01e6bfa24d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SPLITS = 5\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ed9dd21-8f9b-40bb-accc-dd6cc7bfc903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[TabNet] Fold 1\n",
      "\n",
      "Early stopping occurred at epoch 87 with best_epoch = 67 and best_val_0_auc = 0.84931\n",
      "\n",
      "[TabNet] Fold 2\n",
      "\n",
      "Early stopping occurred at epoch 56 with best_epoch = 36 and best_val_0_auc = 0.84603\n",
      "\n",
      "[TabNet] Fold 3\n",
      "\n",
      "Early stopping occurred at epoch 84 with best_epoch = 64 and best_val_0_auc = 0.84697\n",
      "\n",
      "[TabNet] Fold 4\n",
      "\n",
      "Early stopping occurred at epoch 83 with best_epoch = 63 and best_val_0_auc = 0.85063\n",
      "\n",
      "[TabNet] Fold 5\n",
      "\n",
      "Early stopping occurred at epoch 57 with best_epoch = 37 and best_val_0_auc = 0.85555\n",
      "TabNet CV AUC: 0.849041017810293\n"
     ]
    }
   ],
   "source": [
    "oof_tabnet = np.zeros(len(df_train_fe))\n",
    "test_tabnet = np.zeros(len(df_test_fe))\n",
    "\n",
    "for fold, (tr_idx, va_idx) in enumerate(skf.split(df_train_fe, df_train_fe[TARGET])):\n",
    "    print(f\"\\n[TabNet] Fold {fold+1}\")\n",
    "\n",
    "    tr_df = df_train_fe.iloc[tr_idx].copy()\n",
    "    va_df = df_train_fe.iloc[va_idx].copy()\n",
    "    te_df = df_test_fe.copy()\n",
    "\n",
    "    tr_df, va_df, te_df = apply_rankgauss_fold(\n",
    "        tr_df, va_df, te_df, TABNET_LOG_COLS\n",
    "    )\n",
    "\n",
    "    X_tr = tr_df[FEATURE_COLS_TABNET].apply(pd.to_numeric, errors=\"coerce\")\n",
    "    X_va = va_df[FEATURE_COLS_TABNET].apply(pd.to_numeric, errors=\"coerce\")\n",
    "    X_te = te_df[FEATURE_COLS_TABNET].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "    med = X_tr.median()\n",
    "    X_tr = X_tr.fillna(med).values.astype(np.float32)\n",
    "    X_va = X_va.fillna(med).values.astype(np.float32)\n",
    "    X_te = X_te.fillna(med).values.astype(np.float32)\n",
    "\n",
    "    y_tr = tr_df[TARGET].values\n",
    "    y_va = va_df[TARGET].values\n",
    "\n",
    "    model = TabNetClassifier(\n",
    "        n_d=32, n_a=32,\n",
    "        n_steps=5,\n",
    "        gamma=1.5,\n",
    "        lambda_sparse=1e-4,\n",
    "        optimizer_fn=torch.optim.Adam,\n",
    "        optimizer_params=dict(lr=2e-2),\n",
    "        mask_type=\"entmax\",\n",
    "        seed=SEED,\n",
    "        device_name=DEVICE,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_tr, y_tr,\n",
    "        eval_set=[(X_va, y_va)],\n",
    "        eval_metric=[\"auc\"],\n",
    "        max_epochs=150,\n",
    "        patience=20,\n",
    "        batch_size=1024,\n",
    "        virtual_batch_size=128,\n",
    "        num_workers=0\n",
    "    )\n",
    "\n",
    "    oof_tabnet[va_idx] = model.predict_proba(X_va)[:, 1]\n",
    "    test_tabnet += model.predict_proba(X_te)[:, 1] / N_SPLITS\n",
    "\n",
    "print(\"TabNet CV AUC:\", roc_auc_score(df_train_fe[TARGET], oof_tabnet))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de4a5f0a-8cdb-449a-87ad-ad656ea80bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[LGBM] Fold 1\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[729]\tvalid_0's auc: 0.854549\n",
      "\n",
      "[LGBM] Fold 2\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[585]\tvalid_0's auc: 0.852865\n",
      "\n",
      "[LGBM] Fold 3\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[556]\tvalid_0's auc: 0.851317\n",
      "\n",
      "[LGBM] Fold 4\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[397]\tvalid_0's auc: 0.855285\n",
      "\n",
      "[LGBM] Fold 5\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[453]\tvalid_0's auc: 0.862287\n",
      "LGBM CV AUC: 0.8551638507403657\n"
     ]
    }
   ],
   "source": [
    "oof_lgb = np.zeros(len(df_train_fe))\n",
    "test_lgb = np.zeros(len(df_test_fe))\n",
    "\n",
    "X_all = df_train_fe[FEATURE_COLS_LGB].apply(pd.to_numeric, errors=\"coerce\")\n",
    "X_all = X_all.fillna(X_all.median()).values\n",
    "y_all = df_train_fe[TARGET].values\n",
    "\n",
    "X_test_lgb = df_test_fe[FEATURE_COLS_LGB].apply(pd.to_numeric, errors=\"coerce\")\n",
    "X_test_lgb = X_test_lgb.fillna(\n",
    "    df_train_fe[FEATURE_COLS_LGB].median()\n",
    ").values\n",
    "\n",
    "lgb_params = {\n",
    "    \"objective\": \"binary\",\n",
    "    \"metric\": \"auc\",\n",
    "    \"learning_rate\": 0.03,\n",
    "    \"num_leaves\": 31,\n",
    "    \"min_data_in_leaf\": 30,\n",
    "    \"feature_fraction\": 0.8,\n",
    "    \"bagging_fraction\": 0.8,\n",
    "    \"bagging_freq\": 1,\n",
    "    \"lambda_l2\": 1.0,\n",
    "    \"verbosity\": -1,\n",
    "    \"seed\": SEED,\n",
    "}\n",
    "\n",
    "for fold, (tr_idx, va_idx) in enumerate(skf.split(X_all, y_all)):\n",
    "    print(f\"\\n[LGBM] Fold {fold+1}\")\n",
    "\n",
    "    model = lgb.LGBMClassifier(**lgb_params, n_estimators=5000)\n",
    "\n",
    "    model.fit(\n",
    "        X_all[tr_idx], y_all[tr_idx],\n",
    "        eval_set=[(X_all[va_idx], y_all[va_idx])],\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(100),\n",
    "            lgb.log_evaluation(0)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    oof_lgb[va_idx] = model.predict_proba(X_all[va_idx])[:, 1]\n",
    "    test_lgb += model.predict_proba(X_test_lgb)[:, 1] / N_SPLITS\n",
    "\n",
    "print(\"LGBM CV AUC:\", roc_auc_score(y_all, oof_lgb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8bbcde70-431f-4ddc-831e-e491b9c34cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[LGBM-safe] Fold 1\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[975]\tvalid_0's auc: 0.854535\n",
      "\n",
      "[LGBM-safe] Fold 2\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[748]\tvalid_0's auc: 0.852122\n",
      "\n",
      "[LGBM-safe] Fold 3\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1298]\tvalid_0's auc: 0.852388\n",
      "\n",
      "[LGBM-safe] Fold 4\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[847]\tvalid_0's auc: 0.855437\n",
      "\n",
      "[LGBM-safe] Fold 5\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[744]\tvalid_0's auc: 0.86146\n",
      "LGBM-safe CV AUC: 0.8551096832806264\n"
     ]
    }
   ],
   "source": [
    "oof_lgb2 = np.zeros(len(df_train_fe))\n",
    "test_lgb2 = np.zeros(len(df_test_fe))\n",
    "\n",
    "# ★ 保守型パラメータ（性格を変える）\n",
    "lgb_params_safe = {\n",
    "    \"objective\": \"binary\",\n",
    "    \"metric\": \"auc\",\n",
    "    \"learning_rate\": 0.02,\n",
    "    \"num_leaves\": 16,          # 小さめ\n",
    "    \"min_data_in_leaf\": 60,    # 大きめ\n",
    "    \"feature_fraction\": 0.7,\n",
    "    \"bagging_fraction\": 0.7,\n",
    "    \"bagging_freq\": 1,\n",
    "    \"lambda_l1\": 0.5,          # 正則化強め\n",
    "    \"lambda_l2\": 2.0,\n",
    "    \"verbosity\": -1,\n",
    "    \"seed\": SEED,\n",
    "}\n",
    "\n",
    "for fold, (tr_idx, va_idx) in enumerate(skf.split(X_all, y_all)):\n",
    "    print(f\"\\n[LGBM-safe] Fold {fold+1}\")\n",
    "\n",
    "    model = lgb.LGBMClassifier(**lgb_params_safe, n_estimators=5000)\n",
    "\n",
    "    model.fit(\n",
    "        X_all[tr_idx], y_all[tr_idx],\n",
    "        eval_set=[(X_all[va_idx], y_all[va_idx])],\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(100),\n",
    "            lgb.log_evaluation(0)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    oof_lgb2[va_idx] = model.predict_proba(X_all[va_idx])[:, 1]\n",
    "    test_lgb2 += model.predict_proba(X_test_lgb)[:, 1] / N_SPLITS\n",
    "\n",
    "print(\"LGBM-safe CV AUC:\", roc_auc_score(y_all, oof_lgb2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "116acc38-4145-4272-9860-d43168a8ff6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[XGB] Fold 1\n",
      "\n",
      "[XGB] Fold 2\n",
      "\n",
      "[XGB] Fold 3\n",
      "\n",
      "[XGB] Fold 4\n",
      "\n",
      "[XGB] Fold 5\n",
      "XGB CV AUC: 0.8553348278510315\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "oof_xgb = np.zeros(len(df_train_fe))\n",
    "test_xgb = np.zeros(len(df_test_fe))\n",
    "\n",
    "xgb_params = {\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"eval_metric\": \"auc\",\n",
    "    \"learning_rate\": 0.03,\n",
    "    \"max_depth\": 6,\n",
    "    \"min_child_weight\": 30,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"reg_alpha\": 0.0,\n",
    "    \"reg_lambda\": 1.0,\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"random_state\": SEED,\n",
    "}\n",
    "\n",
    "for fold, (tr_idx, va_idx) in enumerate(skf.split(X_all, y_all)):\n",
    "    print(f\"\\n[XGB] Fold {fold+1}\")\n",
    "\n",
    "    model = xgb.XGBClassifier(\n",
    "        **xgb_params,\n",
    "        n_estimators=5000,\n",
    "        early_stopping_rounds=100   # ★ ここに移動\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_all[tr_idx], y_all[tr_idx],\n",
    "        eval_set=[(X_all[va_idx], y_all[va_idx])],\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    oof_xgb[va_idx] = model.predict_proba(X_all[va_idx])[:, 1]\n",
    "    test_xgb += model.predict_proba(X_test_lgb)[:, 1] / N_SPLITS\n",
    "\n",
    "print(\"XGB CV AUC:\", roc_auc_score(y_all, oof_xgb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a10bdea-69b1-4a87-844a-e19d7876eff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEATURE_COLS defined: 29\n",
      "\n",
      "[ResMLP] Fold 1\n",
      "\n",
      "[ResMLP] Fold 2\n",
      "\n",
      "[ResMLP] Fold 3\n",
      "\n",
      "[ResMLP] Fold 4\n",
      "\n",
      "[ResMLP] Fold 5\n",
      "ResMLP CV AUC: 0.8509225164422449\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# ===== FEATURE_COLS 再定義（保険） =====\n",
    "TARGET = \"smoking\"\n",
    "DROP_COLS = [\"id\"] if \"id\" in df_train_fe.columns else []\n",
    "\n",
    "FEATURE_COLS = [\n",
    "    c for c in df_train_fe.columns\n",
    "    if c not in DROP_COLS + [TARGET]\n",
    "]\n",
    "\n",
    "print(\"FEATURE_COLS defined:\", len(FEATURE_COLS))\n",
    "\n",
    "# ===============================\n",
    "# Dataset\n",
    "# ===============================\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, X, y=None):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = None if y is None else torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is None:\n",
    "            return self.X[idx]\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# ===============================\n",
    "# Model\n",
    "# ===============================\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, dim, dropout=0.25):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.BatchNorm1d(dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.BatchNorm1d(dim),\n",
    "        )\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(x + self.net(x))\n",
    "\n",
    "\n",
    "class TabularResMLP(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim=256, n_blocks=4, dropout=0.25):\n",
    "        super().__init__()\n",
    "\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[ResidualBlock(hidden_dim, dropout) for _ in range(n_blocks)]\n",
    "        )\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.blocks(x)\n",
    "        return self.head(x).squeeze(1)\n",
    "\n",
    "# ===============================\n",
    "# Training / Evaluation\n",
    "# ===============================\n",
    "def train_one_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict(model, loader):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "\n",
    "    for batch in loader:\n",
    "        # batch が tuple / list の場合に対応\n",
    "        if isinstance(batch, (list, tuple)):\n",
    "            xb = batch[0]\n",
    "        else:\n",
    "            xb = batch\n",
    "\n",
    "        xb = xb.to(DEVICE)\n",
    "        logits = model(xb)\n",
    "        preds.append(torch.sigmoid(logits).cpu().numpy())\n",
    "\n",
    "    return np.concatenate(preds)\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# ResMLP K-Fold CV\n",
    "# ===============================\n",
    "oof_resmlp = np.zeros(len(df_train_fe))\n",
    "test_resmlp = np.zeros(len(df_test_fe))\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "EPOCHS = 50\n",
    "PATIENCE = 10\n",
    "\n",
    "X_all = df_train_fe[FEATURE_COLS].apply(pd.to_numeric, errors=\"coerce\")\n",
    "X_all = X_all.fillna(X_all.median()).values\n",
    "y_all = df_train_fe[TARGET].values\n",
    "\n",
    "X_test_all = df_test_fe[FEATURE_COLS].apply(pd.to_numeric, errors=\"coerce\")\n",
    "X_test_all = X_test_all.fillna(\n",
    "    df_train_fe[FEATURE_COLS].median()\n",
    ").values\n",
    "\n",
    "for fold, (tr_idx, va_idx) in enumerate(skf.split(X_all, y_all)):\n",
    "    print(f\"\\n[ResMLP] Fold {fold+1}\")\n",
    "\n",
    "    X_tr, X_va = X_all[tr_idx], X_all[va_idx]\n",
    "    y_tr, y_va = y_all[tr_idx], y_all[va_idx]\n",
    "\n",
    "    # 標準化（fold内fit）\n",
    "    scaler = StandardScaler()\n",
    "    X_tr = scaler.fit_transform(X_tr)\n",
    "    X_va = scaler.transform(X_va)\n",
    "    X_te = scaler.transform(X_test_all)\n",
    "\n",
    "    train_ds = TabularDataset(X_tr, y_tr)\n",
    "    val_ds   = TabularDataset(X_va, y_va)\n",
    "    test_ds  = TabularDataset(X_te)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_loader  = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    model = TabularResMLP(\n",
    "        in_dim=X_tr.shape[1],\n",
    "        hidden_dim=256,\n",
    "        n_blocks=4,\n",
    "        dropout=0.25,\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    # クラス不均衡対応\n",
    "    pos = (y_tr == 1).sum()\n",
    "    neg = (y_tr == 0).sum()\n",
    "    pos_weight = torch.tensor([neg / pos], device=DEVICE)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-3, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "    best_auc = -1\n",
    "    patience = 0\n",
    "    best_state = None\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "\n",
    "        # validation AUC\n",
    "        val_preds = predict(model, val_loader)\n",
    "        val_auc = roc_auc_score(y_va, val_preds)\n",
    "        scheduler.step()\n",
    "\n",
    "        if val_auc > best_auc:\n",
    "            best_auc = val_auc\n",
    "            best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience >= PATIENCE:\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "\n",
    "    # OOF\n",
    "    oof_resmlp[va_idx] = predict(model, val_loader)\n",
    "\n",
    "    # test\n",
    "    test_resmlp += predict(model, test_loader) / N_SPLITS\n",
    "\n",
    "print(\"ResMLP CV AUC:\", roc_auc_score(y_all, oof_resmlp))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85c4868f-2195-4d1a-a079-09e17bc65104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble (5 models) CV AUC: 0.8570729257337946\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ensemble_oof_5 = (\n",
    "    oof_tabnet +\n",
    "    oof_lgb +\n",
    "    oof_lgb2 +\n",
    "    oof_xgb +\n",
    "    oof_resmlp\n",
    ") / 5\n",
    "\n",
    "ensemble_test_5 = (\n",
    "    test_tabnet +\n",
    "    test_lgb +\n",
    "    test_lgb2 +\n",
    "    test_xgb +\n",
    "    test_resmlp\n",
    ") / 5\n",
    "\n",
    "print(\"Ensemble (5 models) CV AUC:\", roc_auc_score(y_all, ensemble_oof_5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe631266-c701-49ec-b149-2601e68045af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>smoking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15000</td>\n",
       "      <td>0.018891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15001</td>\n",
       "      <td>0.564897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15002</td>\n",
       "      <td>0.111995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15003</td>\n",
       "      <td>0.298498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15004</td>\n",
       "      <td>0.145780</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id   smoking\n",
       "0  15000  0.018891\n",
       "1  15001  0.564897\n",
       "2  15002  0.111995\n",
       "3  15003  0.298498\n",
       "4  15004  0.145780"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.DataFrame({\n",
    "    \"id\": test_ids,\n",
    "    \"smoking\": ensemble_test_5\n",
    "})\n",
    "\n",
    "submission.to_csv(\"submission_ensemble_5models.csv\", index=False)\n",
    "submission.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9943f6-57f9-40af-9018-78f61cbba566",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18256158-2977-4c27-ac51-03841b83d848",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1f914e-8519-48c2-a078-0c00c41697a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd736b23-c144-493c-8a4f-55e388b96b90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3af8652-2392-4dc8-a694-3747bcac547c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58e1455-b507-4f87-9267-a5a15068378e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
